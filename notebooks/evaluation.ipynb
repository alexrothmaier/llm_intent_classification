{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5734edf1-fc84-4487-bddb-441550b103a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42548062a6f146e0b99e436443be1325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It seems like you just typed a random word \"test\". Is there something specific you\\'d like to test or discuss? I\\'m here to help with any questions or topics you\\'d like to explore!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data from csv\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "from src.llm_alex import Llama\n",
    "from langchain_core import pydantic_v1\n",
    "from langchain_core.runnables.base import RunnableParallel, RunnableLambda\n",
    "from langchain.output_parsers.retry import RetryOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from typing import List, Optional, Any, Union\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random \n",
    "from time import time \n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "test_data = pd.read_csv(\"data/test_data.csv\")\n",
    "\n",
    "llm = Llama()\n",
    "llm(query=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c8cfe0-f6d4-4c25-ad8f-5d5be45e9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ Helper Methods ------------\n",
    "def get_prompt_length(llm, str_input):\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": llm._system_msg},\n",
    "            {\"role\": \"user\", \"content\": str_input},\n",
    "        ]\n",
    "    prompt = llm.pipeline.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens_length = len(llm.pipeline.tokenizer.encode(prompt))\n",
    "    prompt_length = len(prompt)\n",
    "    return input_tokens_length,prompt_length\n",
    "def process_row(row):\n",
    "    start = time() \n",
    "    try:\n",
    "        row['prediction'] = main_chain.invoke({\"text\": row['text']})\n",
    "    except ValueError as e:\n",
    "        #print(\"Value Error encountered\")\n",
    "        row['prediction'] = np.nan\n",
    "    end = time()\n",
    "    row['inference_time'] = end-start\n",
    "    input_tokens_length, prompt_length = get_prompt_length(llm, few_shot_prompt.format(text=row['text']))\n",
    "    row['input_tokens_length'] = input_tokens_length\n",
    "    row['prompt_length'] = prompt_length\n",
    "    row['prompt'] = few_shot_prompt.format(text=row['text'])\n",
    "    return row\n",
    "def get_examples(df, num_shots):\n",
    "    examples = []\n",
    "    for label in df.label.unique():\n",
    "        for i, row in df[df['label'] == label].sample(num_shots, random_state=random_seed).iterrows():\n",
    "            examples.append({\n",
    "                \"input\": row['text'],\n",
    "                \"output\": str(label)\n",
    "            })\n",
    "            df = df.drop(i)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe4bfa-6cec-4ce0-8d18-4816cbd8c687",
   "metadata": {},
   "source": [
    "## Naive Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4ffc03-81fb-42a3-9ceb-aa60b83a73f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f0d59a12dd4cbca3f3af3223cd7593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing n_classes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6ae7c67b324e11bd2819374c5d0c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "#craft examples\n",
    "tqdm.pandas()\n",
    "\n",
    "random.seed(42)\n",
    "random_seed = 42\n",
    "n_classes_list = [40]#,30,50,76]\n",
    "num_shots_list = [1]\n",
    "\n",
    "for n_classes in tqdm(n_classes_list, desc=\"Processing n_classes\"):\n",
    "    for num_shots in tqdm(num_shots_list, desc=\"Processing num_shots\", leave=False):\n",
    "        #sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed, replace=False).values\n",
    "        sampled_classes = random.sample(test_data['label'].unique().tolist(),n_classes)\n",
    "        train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "        test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "        #print(\"*\"*50)\n",
    "        #print(f\"Number of sampled classes: {len(sampled_classes)}\")\n",
    "        #print(f\"Number of samples in test_data_sub: {len(test_data_sub)}\")\n",
    "\n",
    "\n",
    "        #Achtung: muss hier drin stehen, da sampled_classes verwendet wird. Geht nicht sauberer\n",
    "        class NumberParser(BaseOutputParser):\n",
    "            def parse(self, text: str) -> Any:\n",
    "                try:\n",
    "                    # Extracting the first number from the text\n",
    "                    number = int(text.strip())\n",
    "                    if number not in sampled_classes:\n",
    "                        raise ValueError(f\"Number {number} is not in the list of valid classes.\")\n",
    "                    return number\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Failed to parse number: {e}\")\n",
    "        \n",
    "\n",
    "        examples = get_examples(test_data_sub, num_shots)\n",
    "        example_prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"output\"], template=\"input: {input}\\noutput: {output}\"\n",
    "        )\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\",\n",
    "            examples=examples,\n",
    "            example_prompt=example_prompt,\n",
    "            suffix=\"input: {text}.\\noutput:\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "        completion_chain = few_shot_prompt | llm\n",
    "        \n",
    "        custom_parser = NumberParser()\n",
    "        retry_parser = RetryOutputParser(parser=custom_parser, max_retries=3)\n",
    "        \n",
    "        main_chain = RunnableParallel(\n",
    "            completion=completion_chain, prompt_value=few_shot_prompt\n",
    "        ) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "        \n",
    "        \n",
    "        test_data_sub = test_data_sub.progress_apply(process_row, axis=1)\n",
    "\n",
    "        #-------- Evaluate and log results -----------\n",
    "        file_name = f\"data/few_shot/test/classes_{n_classes}_shots_{num_shots}\"\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "        test_data_sub.to_csv(os.path.join(file_name,\"predictions.csv\"),index=False)\n",
    "        nan_count = test_data_sub['prediction'].isna().sum()\n",
    "        test_data_sub['prediction'] = test_data_sub['prediction'].fillna(-1)\n",
    "        #print(f\"Number of NaN values in 'prediction': {nan_count}\")\n",
    "        report = classification_report(test_data_sub['label'], test_data_sub['prediction'], output_dict=True, zero_division=np.nan)\n",
    "        with open(os.path.join(file_name,\"results.json\"), \"w\") as f:\n",
    "            json.dump(report, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fc161-fe8e-450b-b05f-ab345c879f64",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d8ba5d-cd9d-41b4-86b6-420a2e24f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"data/few_shot/test/classes_40_shots_1\"\n",
    "predictions = pd.read_csv(os.path.join(dir_name,\"predictions.csv\"), index_col=False)\n",
    "with open(os.path.join(dir_name,\"results.json\"), \"r\") as f:\n",
    "        report = json.load(f)\n",
    "predictions\n",
    "len(predictions[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2caddd5-38dd-4490-beef-9056ff53bb86",
   "metadata": {},
   "source": [
    "## Cosine Similarity Example Selection\n",
    "\n",
    "Idee: wähle few shot examples basierend auf der similarity zur query aus -> reduziere damit kontextlänge.\n",
    "- FAISS vector storage\n",
    "- HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f56cf29-70bd-4fff-a6cb-db6dbdfba1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f66a86-75d6-405e-aacd-fd4cf703e1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec344037877a4494b08f15a6e33c40a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing n_classes:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb134dac3b74d319fec833fd4af6b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095ceb0f691b4306bccba4c4b0dd8948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3448f474bf04330bce8a0d9bde1290f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f2000d660940fc9e37d4c435ffe26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ed9ec8b2994f4fb31d96fe1bfad6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732b67f8a1384432a8494915fa64de08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "random_seed = 42\n",
    "random.seed(42)\n",
    "n_classes_list = [5,10,20,30,40,50]\n",
    "num_shots_list = [1]\n",
    "k=1\n",
    "\n",
    "for n_classes in tqdm(n_classes_list, desc=\"Processing n_classes\"):\n",
    "    for num_shots in tqdm(num_shots_list, desc=\"Processing num_shots\", leave=False):\n",
    "        #sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "        sampled_classes = random.sample(test_data['label'].unique().tolist(),n_classes)\n",
    "        train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "        test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "        #print(\"*\"*50)\n",
    "        #print(f\"Number of sampled classes: {len(sampled_classes)}\")\n",
    "        #print(f\"Number of samples in test_data_sub: {len(test_data_sub)}\")\n",
    "\n",
    "\n",
    "        #Achtung: muss hier drin stehen, da sampled_classes verwendet wird. Geht nicht sauberer\n",
    "        class NumberParser(BaseOutputParser):\n",
    "            def parse(self, text: str) -> Any:\n",
    "                try:\n",
    "                    # Extracting the first number from the text\n",
    "                    number = int(text.strip())\n",
    "                    if number not in sampled_classes:\n",
    "                        raise ValueError(f\"Number {number} is not in the list of valid classes.\")\n",
    "                    return number\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Failed to parse number: {e}\")\n",
    "        \n",
    "\n",
    "        examples = get_examples(test_data_sub, num_shots)\n",
    "        example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "            examples,\n",
    "            hf,\n",
    "            FAISS,\n",
    "            k=k,\n",
    "        )\n",
    "        example_prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"output\"], template=\"input: {input}\\noutput: {output}\"\n",
    "        )\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\",\n",
    "            example_selector=example_selector,\n",
    "            example_prompt=example_prompt,\n",
    "            suffix=\"input: {text}.\\noutput:\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "        completion_chain = few_shot_prompt | llm\n",
    "        \n",
    "        custom_parser = NumberParser()\n",
    "        retry_parser = RetryOutputParser(parser=custom_parser, max_retries=3)\n",
    "        \n",
    "        main_chain = RunnableParallel(\n",
    "            completion=completion_chain, prompt_value=few_shot_prompt\n",
    "        ) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "        \n",
    "        \n",
    "        test_data_sub = test_data_sub.progress_apply(process_row, axis=1)\n",
    "\n",
    "        #-------- Evaluate and log results -----------\n",
    "        file_name = f\"data/enhanced_few_shot/test/classes_{n_classes}_faiss_{k}\"\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "        test_data_sub.to_csv(os.path.join(file_name,\"predictions.csv\"), index=False)\n",
    "        nan_count = test_data_sub['prediction'].isna().sum()\n",
    "        test_data_sub['prediction'] = test_data_sub['prediction'].fillna(-1)\n",
    "        #print(f\"Number of NaN values in 'prediction': {nan_count}\")\n",
    "        report = classification_report(test_data_sub['label'], test_data_sub['prediction'], output_dict=True, zero_division=np.nan)\n",
    "        with open(os.path.join(file_name,\"results.json\"), \"w\") as f:\n",
    "            json.dump(report, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd324eb-73b7-4f2e-915f-1bdea89961f6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20f6fad-6ff9-4026-ac8f-d18d2bc559c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"data/enhanced_few_shot/test/classes_10_faiss_3\"\n",
    "predictions = pd.read_csv(os.path.join(dir_name,\"predictions.csv\"), index_col=False)\n",
    "with open(os.path.join(dir_name,\"results.json\"), \"r\") as f:\n",
    "        report = json.load(f)\n",
    "len(predictions[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850181c-b164-4268-8fd5-e0096181f254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ef204ad-66eb-4755-a9e2-3c9394532558",
   "metadata": {},
   "source": [
    "## MMR Example Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1477e5-2802-400c-8032-d0f85a0dc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b275f7c4-ef00-44d7-bc21-29a7f4d32948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caee250cd3b644f19a8391dafccf51e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing n_classes:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b85c283dd1e45ffa590284b93b6156f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd43f28278e44da19dc2e57cde4eb8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5366d20740149e69e4074824e4857e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f5ad6cd2ec452ab368dce75986a8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81799b01bdd041ae97a94232b529d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca7dac9309047478fea2bb3e4bce6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "random_seed = 42\n",
    "random.seed(42)\n",
    "n_classes_list = [5,10,20,30,40,50]\n",
    "num_shots_list = [1]\n",
    "k=10\n",
    "\n",
    "for n_classes in tqdm(n_classes_list, desc=\"Processing n_classes\"):\n",
    "    for num_shots in tqdm(num_shots_list, desc=\"Processing num_shots\", leave=False):\n",
    "        sampled_classes = random.sample(test_data['label'].unique().tolist(),n_classes)\n",
    "        train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "        test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "\n",
    "        #Achtung: muss hier drin stehen, da sampled_classes verwendet wird. Geht nicht sauberer\n",
    "        class NumberParser(BaseOutputParser):\n",
    "            def parse(self, text: str) -> Any:\n",
    "                try:\n",
    "                    # Extracting the first number from the text\n",
    "                    number = int(text.strip())\n",
    "                    if number not in sampled_classes:\n",
    "                        raise ValueError(f\"Number {number} is not in the list of valid classes.\")\n",
    "                    return number\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Failed to parse number: {e}\")\n",
    "        \n",
    "\n",
    "        examples = get_examples(test_data_sub, num_shots)\n",
    "        example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "            examples,\n",
    "            hf,\n",
    "            FAISS,\n",
    "            k=k,\n",
    "        )\n",
    "        example_prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"output\"], template=\"input: {input}\\noutput: {output}\"\n",
    "        )\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\",\n",
    "            example_selector=example_selector,\n",
    "            example_prompt=example_prompt,\n",
    "            suffix=\"input: {text}.\\noutput:\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "        completion_chain = few_shot_prompt | llm\n",
    "        \n",
    "        custom_parser = NumberParser()\n",
    "        retry_parser = RetryOutputParser(parser=custom_parser, max_retries=3)\n",
    "        \n",
    "        main_chain = RunnableParallel(\n",
    "            completion=completion_chain, prompt_value=few_shot_prompt\n",
    "        ) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "        \n",
    "        \n",
    "        test_data_sub = test_data_sub.progress_apply(process_row, axis=1)\n",
    "\n",
    "        #-------- Evaluate and log results -----------\n",
    "        file_name = f\"data/mmr_few_shot/test/classes_{n_classes}_faiss_{k}\"\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "        test_data_sub.to_csv(os.path.join(file_name,\"predictions.csv\"), index=False)\n",
    "        nan_count = test_data_sub['prediction'].isna().sum()\n",
    "        test_data_sub['prediction'] = test_data_sub['prediction'].fillna(-1)\n",
    "        report = classification_report(test_data_sub['label'], test_data_sub['prediction'], output_dict=True, zero_division=np.nan)\n",
    "        with open(os.path.join(file_name,\"results.json\"), \"w\") as f:\n",
    "            json.dump(report, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec17c82-ea9f-4aff-8894-241655808ced",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465e41b7-162c-40f3-b675-2d2ebebd03d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = \"data/mmr_few_shot/test/classes_5_faiss_1\"\n",
    "predictions = pd.read_csv(os.path.join(dir_name,\"predictions.csv\"), index_col=False)\n",
    "with open(os.path.join(dir_name,\"results.json\"), \"r\") as f:\n",
    "        report = json.load(f)\n",
    "len(predictions[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec9c43-40f6-48c3-8d24-c359ad4161ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391574f-bf18-470d-a217-78fa9a8a0f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfcbbf-31f3-4e91-b48a-07dd3f7b48de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3e02b-74ad-4e9e-a376-bfd454d59c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e3a76f-29fd-4ea6-ab39-005050dbc598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples most similar to the input: Why does my credit card not work?\n",
      "\n",
      "\n",
      "text: Can you tell me why my card keeps getting declined every time I try to use it?\n",
      "label: 25\n",
      "\n",
      "\n",
      "text: Why do you keep declining my payment? I tried several times already with this card and it is just not working.\n",
      "label: 25\n"
     ]
    }
   ],
   "source": [
    "# TEST the example selector\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "text = \"Why does my credit card not work?\"\n",
    "selected_examples = example_selector.select_examples({\"text\": text})\n",
    "print(f\"Examples most similar to the input: {text}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8042963-ee20-4a0c-a494-9b18f922d3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Can you tell me why my card keeps getting declined every time I try to use it?\n",
      "Label: 25\n",
      "\n",
      "Text: Why do you keep declining my payment? I tried several times already with this card and it is just not working.\n",
      "Label: 25\n",
      "\n",
      "Text: Why does my credit card not work?\n"
     ]
    }
   ],
   "source": [
    "enhanced_few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Text: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "print(enhanced_few_shot_prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3589657-237a-4157-9d41-7e759e840e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "n_classes_list = [1]\n",
    "num_shots_list = [1]\n",
    "\n",
    "for n_classes in tqdm(n_classes_list, desc=\"Processing n_classes\"):\n",
    "    for num_shots in tqdm(num_shots_list, desc=\"Processing num_shots\", leave=False):\n",
    "        sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "        train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "        test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "        #print(\"*\"*50)\n",
    "        #print(f\"Number of sampled classes: {len(sampled_classes)}\")\n",
    "        #print(f\"Number of samples in test_data_sub: {len(test_data_sub)}\")\n",
    "\n",
    "\n",
    "        #Achtung: muss hier drin stehen, da sampled_classes verwendet wird. Geht nicht sauberer\n",
    "        class NumberParser(BaseOutputParser):\n",
    "            def parse(self, text: str) -> Any:\n",
    "                try:\n",
    "                    # Extracting the first number from the text\n",
    "                    number = int(text.strip())\n",
    "                    if number not in sampled_classes:\n",
    "                        raise ValueError(f\"Number {number} is not in the list of valid classes.\")\n",
    "                    return number\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Failed to parse number: {e}\")\n",
    "        \n",
    "\n",
    "        examples = get_examples(test_data_sub, num_shots)\n",
    "        example_prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"output\"], template=\"input: {input}\\noutput: {output}\"\n",
    "        )\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\",\n",
    "            examples=examples,\n",
    "            example_selector=example_selector,\n",
    "            example_prompt=example_prompt,\n",
    "            suffix=\"input: {text}.\\noutput:\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "        completion_chain = few_shot_prompt | llm\n",
    "        \n",
    "        custom_parser = NumberParser()\n",
    "        retry_parser = RetryOutputParser(parser=custom_parser, max_retries=3)\n",
    "        \n",
    "        main_chain = RunnableParallel(\n",
    "            completion=completion_chain, prompt_value=few_shot_prompt\n",
    "        ) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "        \n",
    "        \n",
    "        test_data_sub = test_data_sub.apply(process_row, axis=1)\n",
    "\n",
    "        #-------- Evaluate and log results -----------\n",
    "        file_name = f\"data/enhanced_few_shot/test/classes_{n_classes}_shots_{num_shots}\"\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "        test_data_sub.to_csv(os.path.join(file_name,\"predictions.csv\"))\n",
    "        nan_count = test_data_sub['prediction'].isna().sum()\n",
    "        test_data_sub['prediction'] = test_data_sub['prediction'].fillna(0)\n",
    "        #print(f\"Number of NaN values in 'prediction': {nan_count}\")\n",
    "        report = classification_report(test_data_sub['label'], test_data_sub['prediction'], output_dict=True, zero_division=np.nan)\n",
    "        with open(os.path.join(file_name,\"results.json\"), \"w\") as f:\n",
    "            json.dump(report, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f65567b-7bb7-4fbf-8857-5bf0c22ce967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help you troubleshoot the issue with your credit card!\\n\\nThere could be several reasons why your card is being declined. Here are a few common causes:\\n\\n1. **Insufficient funds**: Make sure you have enough available credit on your card to cover the transaction.\\n2. **Expired card**: Check the expiration date on your card to ensure it's still valid.\\n3. **Invalid card information**: Double-check that you're entering the correct card number, expiration date, and security code.\\n4. **Card issuer restrictions**: Your card issuer might have placed a hold on your account or restricted certain types of transactions.\\n5. **Card security features**: Some cards have security features that can cause transactions to be declined, such as a chip or a digital signature.\\n6. **Merchant-specific issues**: The merchant's payment processing system might be experiencing technical difficulties or have specific requirements for card acceptance.\\n\\nTo resolve the issue, you can try the following:\\n\\n1. Contact your card issuer's customer support to report the issue and ask for assistance.\\n2. Check your account balance and available credit to ensure you have sufficient funds.\\n3. Try using a different payment method, such as a different credit card or a debit card.\\n4. Contact the merchant's customer support to\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(query=enhanced_few_shot_prompt.format(text=text))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c78986-86a3-4216-83a9-43d49174433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_length(llm, str_input):\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": llm._system_msg},\n",
    "            {\"role\": \"user\", \"content\": str_input},\n",
    "        ]\n",
    "    prompt = llm.pipeline.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens_length = len(llm.pipeline.tokenizer.encode(prompt))\n",
    "    prompt_length = len(prompt)\n",
    "    return input_tokens_length,prompt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3a4004-af12-4739-8dd4-f8cd33a539a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens_length, prompt_length = get_prompt_length(llm, enhanced_few_shot_prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41624151-4f5a-4595-850e-493158493a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d41839bb-dcdc-4ead-8efa-e5f300a95799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b98c48b-b924-4245-acbe-0d6cb7735df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text: Why do you keep declining my payment? I tried several times already with this card and it is just not working.\\nLabel: 25\\n\\nText: What is the reason my payment was not accepted?\\nLabel: 25\\n\\nText: Your task is to classify a given text into one of the following classes, reply ONLY with the class label: \\n\\nLabel: 25\\nText: Why are you declining my payment? Everything was fine.\\nText: I have a card payment that was declined, but why?\\n\\nLabel: 21\\nText: I would like to change my pin.\\nText: How can I change my Tholepin ?\\n\\nHere is your text, please classify it into one of the above classes\\n\\nText: my credit card does not work'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_few_shot_prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25055701-96df-46a6-a3cc-2d1d5893188d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "1045\n"
     ]
    }
   ],
   "source": [
    "class Prediction(BaseModel):\n",
    "    label: int = Field(description=\"classification label to a given text\")\n",
    "    @validator(\"label\")\n",
    "    def label_is_valid(cls, field): \n",
    "        if not (0 <= field <= 76):\n",
    "            raise ValueError(\"label must be an integer between 0 and 76\")\n",
    "        return field\n",
    "parser = PydanticOutputParser(pydantic_object=Prediction)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "enhanced_few_shot_prompt = FewShotPromptTemplate(\n",
    "    prefix=\"Answer the user query.\\n{format_instructions}\\n{text}\\n\",\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Text: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "retry_parser = RetryOutputParser.from_llm(parser=parser, llm=llm, max_retries=3)\n",
    " \n",
    "completion_chain = enhanced_few_shot_prompt | llm\n",
    "\n",
    "main_chain = RunnableParallel(\n",
    "    completion=completion_chain, prompt_value=enhanced_few_shot_prompt\n",
    ") | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "\n",
    "\n",
    "answer: Prediction = main_chain.invoke({\"text\": text})\n",
    "input_tokens_length, prompt_length = get_prompt_length(llm, enhanced_few_shot_prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6adf6b19-5e70-4e32-9690-9c12116cce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d732c48-3a56-4f21-adcd-f2709f6bc2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"label\": {\"title\": \"Label\", \"description\": \"classification label to a given text\", \"type\": \"integer\"}}, \"required\": [\"label\"]}\\n```\\nWhy does my credit card not work?\\n\\n\\nText: Can you tell me why my card keeps getting declined every time I try to use it?\\nLabel: 25\\n\\nText: why couldn\\'t I use my card at a store?\\nLabel: 25\\n\\nText: Why does my credit card not work?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_few_shot_prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae7812-2ba2-427e-b6f8-0b13746a40bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fef2e4-9020-4c2b-83b9-ada0855653e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa9abd6-0b36-413b-861b-8e6b97b58a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'completion'}.  Expected: ['completion', 'prompt'] Received: ['prompt', 'input']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/json.py:66\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/utils/json.py:147\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/utils/json.py:160\u001b[0m, in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/utils/json.py:120\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain/output_parsers/retry.py:90\u001b[0m, in \u001b[0;36mRetryOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/pydantic.py:64\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/json.py:72\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/pydantic.py:60\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[0;32m---> 60\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_obj(json_object)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/json.py:69\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     68\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: Here is the output in the required JSON format:\n\n{\"label\": 25}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:2507\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2506\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2507\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:3985\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3987\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3988\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3989\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3990\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3995\u001b[0m     )\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:1599\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1596\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1597\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1598\u001b[0m         Output,\n\u001b[0;32m-> 1599\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1609\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:3853\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3851\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3853\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3856\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     20\u001b[0m retry_parser \u001b[38;5;241m=\u001b[39m RetryOutputParser\u001b[38;5;241m.\u001b[39mfrom_llm(parser\u001b[38;5;241m=\u001b[39mparser, llm\u001b[38;5;241m=\u001b[39mllm, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     22\u001b[0m completion_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     24\u001b[0m main_chain \u001b[38;5;241m=\u001b[39m RunnableParallel(\n\u001b[1;32m     25\u001b[0m     completion\u001b[38;5;241m=\u001b[39mcompletion_chain, prompt_value\u001b[38;5;241m=\u001b[39mprompt\n\u001b[0;32m---> 26\u001b[0m ) \u001b[38;5;241m|\u001b[39m RunnableLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mretry_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_with_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m answer: Prediction \u001b[38;5;241m=\u001b[39m main_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: text})\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#answer_final: str = answer.label\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#reasoning_final: str = answer.reasoning\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain/output_parsers/retry.py:103\u001b[0m, in \u001b[0;36mRetryOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     98\u001b[0m                     prompt\u001b[38;5;241m=\u001b[39mprompt_value\u001b[38;5;241m.\u001b[39mto_string(),\n\u001b[1;32m     99\u001b[0m                     completion\u001b[38;5;241m=\u001b[39mcompletion,\n\u001b[1;32m    100\u001b[0m                     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(e),\n\u001b[1;32m    101\u001b[0m                 )\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m                 completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:2505\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2501\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2502\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2503\u001b[0m )\n\u001b[1;32m   2504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2505\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2507\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/prompts/base.py:151\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    150\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:1599\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1596\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1597\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1598\u001b[0m         Output,\n\u001b[0;32m-> 1599\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1609\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/prompts/base.py:134\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m--> 134\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/prompts/base.py:126\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    124\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(inner_input)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is missing variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(inner_input\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Input to PromptTemplate is missing variables {'completion'}.  Expected: ['completion', 'prompt'] Received: ['prompt', 'input']\""
     ]
    }
   ],
   "source": [
    "main_chain.invoke({\"query\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe21bbc-3921-445f-b635-d6aac7b22969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17473d9c-38d3-4bfe-b352-9f77d7db2382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca260a6-592b-46db-9e6a-fddbdc913ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab706c-b551-44a3-8208-3f938cf9c8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58951e01-6780-421b-b2a5-48d0aa15aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "n_classes = 2\n",
    "num_shots = 2\n",
    "sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "def get_prompt_template(train_data_sub, num_shots):\n",
    "    prompt_template = \"Your task is to classify a given text into one of the following classes, reply ONLY with the class label: \\n\\n\"\n",
    "    for label in train_data_sub.label.unique():\n",
    "        prompt_template += f\"Label: {label}\\n\"\n",
    "        for i, row in train_data_sub[train_data_sub['label'] == label].sample(num_shots, random_state=random_seed).iterrows():\n",
    "            prompt_template += f\"Text: {row['text']}\\n\"\n",
    "            #remove row from the dataframe\n",
    "            train_data_sub = train_data_sub.drop(i)\n",
    "        prompt_template += \"\\n\"\n",
    "    prompt_template += \"Here is your text, please classify it into one of the above classes\\n\\n\"\n",
    "    return prompt_template\n",
    "\n",
    "#prompt_template = get_prompt_template(test_data_sub,num_shots)\n",
    "\n",
    "text = get_prompt_template(test_data_sub,num_shots)\n",
    "text += \"Text: my credit card does not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f2a9c-f330-4aab-b50c-949cf47db7e8",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f36064-a51a-4ce7-8d5c-0feccef02cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"label\": {\"title\": \"Label\", \"description\": \"classification label to a given text\", \"type\": \"integer\"}}, \"required\": [\"label\"]}\\n```'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "566330cc-fc8a-4083-a410-89774d4b048a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6259c9e661fe4fd7b7b092a940bb0d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "/scratch/slurm_tmpdir/job_23779533/ipykernel_169380/1091179139.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data_sub.loc[i, 'response'] = response\n",
      "Processing data:   1%|▏         | 1/80 [00:16<21:17, 16.18s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|▎         | 2/80 [00:16<09:01,  6.94s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|▍         | 3/80 [00:17<05:06,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|▌         | 4/80 [00:17<03:17,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|▋         | 5/80 [00:18<02:16,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|▊         | 6/80 [00:18<01:40,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   9%|▉         | 7/80 [00:18<01:18,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  10%|█         | 8/80 [00:19<01:03,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  11%|█▏        | 9/80 [00:19<00:53,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  12%|█▎        | 10/80 [00:20<00:46,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  14%|█▍        | 11/80 [00:20<00:41,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  15%|█▌        | 12/80 [00:21<00:38,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  16%|█▋        | 13/80 [00:21<00:35,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  18%|█▊        | 14/80 [00:22<00:33,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  19%|█▉        | 15/80 [00:22<00:32,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  20%|██        | 16/80 [00:23<00:31,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  21%|██▏       | 17/80 [00:23<00:30,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  22%|██▎       | 18/80 [00:24<00:29,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  24%|██▍       | 19/80 [00:24<00:28,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  25%|██▌       | 20/80 [00:25<00:28,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  26%|██▋       | 21/80 [00:25<00:27,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  28%|██▊       | 22/80 [00:25<00:27,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  29%|██▉       | 23/80 [00:26<00:26,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  30%|███       | 24/80 [00:26<00:26,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  31%|███▏      | 25/80 [00:27<00:25,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  32%|███▎      | 26/80 [00:27<00:25,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  34%|███▍      | 27/80 [00:28<00:24,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  35%|███▌      | 28/80 [00:28<00:24,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  36%|███▋      | 29/80 [00:29<00:23,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  38%|███▊      | 30/80 [00:29<00:23,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  39%|███▉      | 31/80 [00:30<00:22,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  40%|████      | 32/80 [00:30<00:22,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  41%|████▏     | 33/80 [00:31<00:21,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  42%|████▎     | 34/80 [00:31<00:21,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  44%|████▍     | 35/80 [00:32<00:20,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  45%|████▌     | 36/80 [00:32<00:20,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  46%|████▋     | 37/80 [00:32<00:20,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  48%|████▊     | 38/80 [00:33<00:19,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  49%|████▉     | 39/80 [00:33<00:19,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  50%|█████     | 40/80 [00:34<00:18,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  51%|█████▏    | 41/80 [00:34<00:18,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  52%|█████▎    | 42/80 [00:35<00:17,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  54%|█████▍    | 43/80 [00:35<00:17,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  55%|█████▌    | 44/80 [00:36<00:16,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  56%|█████▋    | 45/80 [00:36<00:16,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  57%|█████▊    | 46/80 [00:37<00:15,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  59%|█████▉    | 47/80 [00:37<00:15,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  60%|██████    | 48/80 [00:38<00:14,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  61%|██████▏   | 49/80 [00:38<00:14,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  62%|██████▎   | 50/80 [00:39<00:13,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  64%|██████▍   | 51/80 [00:39<00:13,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  65%|██████▌   | 52/80 [00:39<00:13,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  66%|██████▋   | 53/80 [00:40<00:12,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  68%|██████▊   | 54/80 [00:40<00:12,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  69%|██████▉   | 55/80 [00:41<00:11,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  70%|███████   | 56/80 [00:41<00:11,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  71%|███████▏  | 57/80 [00:42<00:10,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  72%|███████▎  | 58/80 [00:42<00:10,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  74%|███████▍  | 59/80 [00:43<00:09,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  75%|███████▌  | 60/80 [00:43<00:09,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  76%|███████▋  | 61/80 [00:44<00:08,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  78%|███████▊  | 62/80 [00:44<00:08,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  79%|███████▉  | 63/80 [00:45<00:07,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  80%|████████  | 64/80 [00:45<00:07,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  81%|████████▏ | 65/80 [00:46<00:06,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  82%|████████▎ | 66/80 [00:46<00:06,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  84%|████████▍ | 67/80 [00:46<00:06,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  85%|████████▌ | 68/80 [00:47<00:05,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  86%|████████▋ | 69/80 [00:47<00:05,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  88%|████████▊ | 70/80 [00:48<00:04,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  89%|████████▉ | 71/80 [00:48<00:04,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  90%|█████████ | 72/80 [00:49<00:03,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  91%|█████████▏| 73/80 [00:49<00:03,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  92%|█████████▎| 74/80 [00:50<00:02,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  94%|█████████▍| 75/80 [00:50<00:02,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  95%|█████████▌| 76/80 [00:51<00:01,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  96%|█████████▋| 77/80 [00:51<00:01,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  98%|█████████▊| 78/80 [00:52<00:00,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  99%|█████████▉| 79/80 [00:52<00:00,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 80/80 [00:53<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, row in tqdm.tqdm(test_data_sub.iterrows(),total=len(test_data_sub), desc=\"Processing data\"):\n",
    "    prompt = prompt_template + f\"Text: {row['text']}\\n\"\n",
    "    response = llm(prompt=prompt)\n",
    "    test_data_sub.loc[i, 'response'] = response\n",
    "    #print(f\"Prompt: {prompt}\")\n",
    "    #print(f\"Response: {response}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cbd4812-0c81-46ca-b8ef-7850ec2b5280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Why won't my card show up on the app?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I would like to reactivate my card.</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Where do I link the new card?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I have received my card, can you help me put i...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How do I link a card that I already have?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>The ATM at Metro bank on High St. Kensington d...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>The ATM won't give back my card</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>I was at an ATM and it swallowed my card.</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>WTF??? I tried to withdraw some money at a Met...</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>What happens if my card is stuck in the ATM?</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label response\n",
       "40                Why won't my card show up on the app?     13       13\n",
       "41                  I would like to reactivate my card.     13       13\n",
       "42                        Where do I link the new card?     13       13\n",
       "43    I have received my card, can you help me put i...     13       13\n",
       "44            How do I link a card that I already have?     13       13\n",
       "...                                                 ...    ...      ...\n",
       "1955  The ATM at Metro bank on High St. Kensington d...     18       18\n",
       "1956                    The ATM won't give back my card     18       18\n",
       "1957          I was at an ATM and it swallowed my card.     18       18\n",
       "1958  WTF??? I tried to withdraw some money at a Met...     18       13\n",
       "1959       What happens if my card is stuck in the ATM?     18       18\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d3b4a6d-e17a-4526-9533-a5bde1b9e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_23779533/ipykernel_169380/3811748138.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data_sub['response'] = test_data_sub['response'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "test_data_sub['response'] = test_data_sub['response'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35afdc67-0dc5-4b13-a34b-241979d318f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          13       0.83      1.00      0.91        40\n",
      "          18       1.00      0.80      0.89        40\n",
      "\n",
      "    accuracy                           0.90        80\n",
      "   macro avg       0.92      0.90      0.90        80\n",
      "weighted avg       0.92      0.90      0.90        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data_sub['label'], test_data_sub['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cc3e231-2808-48c6-9c1c-64d19e432482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          13       0.58      0.88      0.70        40\n",
      "          18       0.75      0.38      0.50        40\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.67      0.62      0.60        80\n",
      "weighted avg       0.67      0.62      0.60        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data_sub['label'], test_data_sub['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94e4a15-3b6a-4409-afb9-e0f7dca60278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required keyword-only argument: 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(test_data_sub\u001b[38;5;241m.\u001b[39miterrows(),total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data_sub), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     26\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt_template \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     test_data_sub\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m response\n\u001b[1;32m     29\u001b[0m test_data_sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data_sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required keyword-only argument: 'query'"
     ]
    }
   ],
   "source": [
    "#iterate over the test data and generate the prompt\n",
    "#surpress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "prompt_template = get_prompt_template(test_data_sub, num_shots)\n",
    "\n",
    "n_classes_list = [5, 10, 20, 30, 40, 50, 77]\n",
    "num_shots = 5\n",
    "results = {}\n",
    "#iterate over the n_classes_list\n",
    "\n",
    "# import wandb \n",
    "# wandb.init(project=\"llm-banking77\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for n_classes in n_classes_list:\n",
    "    sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "    train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "    test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "    prompt_template = get_prompt_template(test_data_sub, num_shots)\n",
    "\n",
    "    for i, row in tqdm.tqdm(test_data_sub.iterrows(),total=len(test_data_sub), desc=\"Processing data\"):\n",
    "        prompt = prompt_template + f\"Text: {row['text']}\\n\"\n",
    "        response = llm(prompt=prompt)\n",
    "        test_data_sub.loc[i, 'response'] = response\n",
    "    test_data_sub['response'] = test_data_sub['response'].label.astype(int)\n",
    "    report = classification_report(test_data_sub['label'], test_data_sub['response'], output_dict=True)\n",
    "    # #print(f\"Classification Report: {classification_report}\")\n",
    "    # #save results to dictionary\n",
    "    # results[n_classes] = {}\n",
    "    results[n_classes] = {\n",
    "        'classification_report': report,\n",
    "        'prompt_template': prompt_template,\n",
    "        'context_length': len(prompt_template)\n",
    "    }\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e63a3-0120-42d6-98eb-f9172250a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca0297-f384-41ed-983c-c8fef0de93e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification_venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
